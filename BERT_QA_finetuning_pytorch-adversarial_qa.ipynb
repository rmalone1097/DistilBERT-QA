{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c0c964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f22c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (C:\\Users\\water\\.cache\\huggingface\\datasets\\adversarial_qa\\adversarialQA\\1.0.0\\92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b869fced63cc432c82b74c8ac914592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\n",
      "    num_rows: 30000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\n",
      "    num_rows: 3000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\n",
      "    num_rows: 3000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "dataset = load_dataset('adversarial_qa', 'adversarialQA')\n",
    "dataset['train'][42]\n",
    "print(dataset['train'])\n",
    "print(dataset['validation'])\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35fd47d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecb3e18fd37445b9e55e582764dcfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f8032442854e6d833fa99d51dc6dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#HuggingFace input preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess(samples):\n",
    "    questions = [q.strip() for q in samples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        samples['context'],\n",
    "        max_length = 384,\n",
    "        truncation = 'only_second',\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    answers = samples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = dataset['train'].map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_validation = dataset['validation'].map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c589441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91fc5c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\water/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\water/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "C:\\Users\\water\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 30000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 55:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.376100</td>\n",
       "      <td>2.914345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.859000</td>\n",
       "      <td>2.406380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.543200</td>\n",
       "      <td>2.084665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.290600</td>\n",
       "      <td>1.853427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.074300</td>\n",
       "      <td>1.604631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.863700</td>\n",
       "      <td>1.424494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.716400</td>\n",
       "      <td>1.312599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.617700</td>\n",
       "      <td>1.201287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.536300</td>\n",
       "      <td>1.136897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.505300</td>\n",
       "      <td>1.124293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3500\n",
      "Configuration saved in ./results\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-4000\n",
      "Configuration saved in ./results\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-4500\n",
      "Configuration saved in ./results\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-5000\n",
      "Configuration saved in ./results\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-5500\n",
      "Configuration saved in ./results\\checkpoint-5500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-5500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-6000\n",
      "Configuration saved in ./results\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-6000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-6500\n",
      "Configuration saved in ./results\\checkpoint-6500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-6500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-7000\n",
      "Configuration saved in ./results\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-7000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-7500\n",
      "Configuration saved in ./results\\checkpoint-7500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-7500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-8000\n",
      "Configuration saved in ./results\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-8000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-8500\n",
      "Configuration saved in ./results\\checkpoint-8500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-8500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-9000\n",
      "Configuration saved in ./results\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-9000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-9500\n",
      "Configuration saved in ./results\\checkpoint-9500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-9500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-10000\n",
      "Configuration saved in ./results\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-10500\n",
      "Configuration saved in ./results\\checkpoint-10500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-10500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-11000\n",
      "Configuration saved in ./results\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-11000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-11500\n",
      "Configuration saved in ./results\\checkpoint-11500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-11500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-12000\n",
      "Configuration saved in ./results\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-12000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-12500\n",
      "Configuration saved in ./results\\checkpoint-12500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-12500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-13000\n",
      "Configuration saved in ./results\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-13000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-13500\n",
      "Configuration saved in ./results\\checkpoint-13500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-13500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-14000\n",
      "Configuration saved in ./results\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-14000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-14500\n",
      "Configuration saved in ./results\\checkpoint-14500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-14500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-15000\n",
      "Configuration saved in ./results\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-15000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-15500\n",
      "Configuration saved in ./results\\checkpoint-15500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-15500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-16000\n",
      "Configuration saved in ./results\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-16000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-16500\n",
      "Configuration saved in ./results\\checkpoint-16500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-16500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-17000\n",
      "Configuration saved in ./results\\checkpoint-17000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-17000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-17500\n",
      "Configuration saved in ./results\\checkpoint-17500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-17500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-18000\n",
      "Configuration saved in ./results\\checkpoint-18000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-18000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./results\\checkpoint-18500\n",
      "Configuration saved in ./results\\checkpoint-18500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-18500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=2.1715930696614585, metrics={'train_runtime': 3317.4055, 'train_samples_per_second': 90.432, 'train_steps_per_second': 5.652, 'total_flos': 2.93969475072e+16, 'train_loss': 2.1715930696614585, 'epoch': 10.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "eval_batch_size = 16\n",
    "\n",
    "train_data_size = len(dataset['train'])\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "initial_learning_rate = 1e-5\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=initial_learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87984f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of its earliest massive implementations was brought about by Egyptians against the British occupation in the 1919 Revolution. Civil disobedience is one of the many ways people have rebelled against what they deem to be unfair laws. It has been used in many nonviolent resistance movements in India (Gandhi's campaigns for independence from the British Empire), in Czechoslovakia's Velvet Revolution and in East Germany to oust their communist governments, In South Africa in the fight against apartheid, in the American Civil Rights Movement, in the Singing Revolution to bring independence to the Baltic countries from the Soviet Union, recently with the 2003 Rose Revolution in Georgia and the 2004 Orange Revolution in Ukraine, among other various movements worldwide.\n",
      "\n",
      "What fabric shares its name with a fight?\n",
      "\n",
      "velvet revolution\n"
     ]
    }
   ],
   "source": [
    "test_sample = dataset['test'][43]\n",
    "test_context = test_sample['context']\n",
    "test_question = test_sample['question']\n",
    "\n",
    "question, text = test_question, test_context\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "inputs.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index:answer_end_index+1]\n",
    "\n",
    "print(test_context)\n",
    "print('')\n",
    "print(test_question)\n",
    "print('')\n",
    "print(tokenizer.decode(predict_answer_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d468fb1a4afb28ced3325811818b75ea47b4773e231d19fb9e9ee0fa4eb4bbba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
